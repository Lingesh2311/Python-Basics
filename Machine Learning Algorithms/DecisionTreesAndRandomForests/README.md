## Random Forests
This is a powerful non-parametric algorithm. This is an example of an ensemble method as it represents the best amongst a set of simple estimators.

Random forests are an example of an ensemble learner built on top of decision trees. The idea behind decision trees is to create a binary split. This effectively reduces the sample size by half during each split. In machine learning implementation, the decision trees are designed to create an axis-aligned split of the data: that is each node in the tree splits the data into two groups using a cut-off value within one of its features.

### Simple Decision Tree
During the alternating axis splits. The tree will iteratively split the data along one or the other axis according to some quantitative criterion, and assign the label based on the majority votes on that region.

The nodes are represented by questions and the corresponding edges are marked by the answers. This results in a child node and the question that follows depends on the answers of the previous branch.

There can be questions based on continuous, discrete and boolean features. There can be more than binary answer generating question but it can always be upscaled to a binary answer question.

This is done using the `sklearn.tree.DecisionTreeClassifier` estimator. 

### Decision Tree and Overfitting
This is one of the major setback of decision trees with increasing depth. As we increase the number of depth layers, we tend to create a chance of fitting the noise in the data rather than the actual data itself.

### Overcoming the Overfitting in Deep Decision Trees
To overcome the overfitting problem in decision trees, we can try to use the information from different trees generated by different subsets of the data.

<em>An ensemble of randomized decision trees is known as the random forests.</em>

### Bagging
This notion- <em>Multiple overfitting estimators combined to reduce the effect of overfitting- is what underlies in an ensemble method called </em> <b>Bagging</b>

<b>Bagging</b> makes use of an ensemble of parallel estimators, each of which overfits the data and averages the result to find a better classification.

This is found in the `sklearn.ensemble.BaggingClassifier` model. This model will try to classify the points based on the randomized splits and ensembling the trees to get the best possible result. However to improve this approach. We can use a stochasticity selection approach to get the random nature factored in. This is done using the `sklearn.ensemble.RandomForestClassifier` model.

This way we get the ensemble of overfitting trees to provide a clearer picture of the classfication.

### Random Forest Regression

This is similar to classification, instead of finding the label via majority number of votes. The model tries to predict the target variable using the splits generated.

This arises from the fact that complex systems have the difficulty of fitting global models (Linear regression). This can be overcome by applying local models on smaller partitions via the application of recursive partitioning.

### Advantages of Random Forests
- Both training and prediction rates are fast. Both tasks can be parallelized, because the induvidual trees are independent entities.
- Multiple trees gives rise to the probability distribution


### Disadvantages of Random Forests
- Results are not easily interpretable. 

### Working of Random Forests
- Take the sample
- Build the ensemble of trees. Each tree is built from a smaller sample but the larger sample is replaced. Consider the sample: 14, 2, 5, 10, 1, 6, 3. If we are asked to draw 3 numbers at random say (2, 3, 5). Then we replace them in the original sample. This is what happens while building the trees. We choose a sample subspace at random with replacement and build a tree. Continue building the `num_estimators` trees. This is one source of randomness.
- The best split is found from either all the features or a subset of them which is defined using `max_features`. This is another source of randomness.
- The purpose of these two sources of randomness is to reduce the variance.
- Combining the diverse trees of high variance will allow the ensemble to have a better prediction accuracy and a <b>reduced variance</b>. But, this comes with a cose of <b>increase in bias</b>.
- `Bootstrap=True`

### Working of Extremely Randomized Trees
- Take the sample
- The randomness goes one step further in the way the splits are computed. Instead of looking at discriminative thresholds, the thresholds are chosen at random for each candidate feature. This allows a <b>better reduction in variance</b> but with a cost of <b>slightly greater increase of bias</b>.
- `Bootstrap=False`

### Parameters
- The main parameters are `n_estimators` and `max_features`. 
- `n_estimators`: The number of trees to consider in the random forests. The larger the better and no larger than the critical number of trees.
- `max_features`: The number of subset of features to consider when splitting a node. The lower the value, greater is the reduction of the variance, but also greater is the increase in bias. For regression set, `max_features=None` (consider all the features). For classification set, `max_features="sqrt"` (using a random subset of size sqrt(features)).

The best parameters are cross-validated.

### Tree Pruning

The size of the tree will determine the accuracy of the classification result. To get the best result from the tree, it is important to prune the branches to get a model that is low in bias and avoids overfitting. This is where the process of tree selection comes into picture.

### Tree Selection

The tree selection is done by calculating the value of the <b>Entropy</b>. This is calculation of the chaocity in a sample space. The entropy is highest when the split is equal. In case of a coin toss event, the entropy is highest when the probability of getting a head or a tail is 0.5.

<b>Combine the adjacent nodes that have low information gain to get a pruned tree.</b>

#### Information Gain

This is the <b>decrease in entropy</b>.
- Calculate the initial Entropy (E1)
- Split the node based on a particular attribute
- Calculate the Entropy (E2)
- Find the decrease in Entropy (E1-E2)
- Repeat this procedure to find the feature that is responsible for the largest decrease in Entropy  or the larget information gain.
- Continue the process to get the features on which the split should follow on the rest of the branches.